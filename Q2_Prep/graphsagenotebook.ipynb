{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560b8c14-1449-42dd-9d62-72e21cd4c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79264b7b-3d7e-49c1-8ebb-e42d101ecf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run - cell containing code for combining graph to two subparts with offset\n",
    "# G_ec = nx.read_graphml(\"ec_graph.graphml\")\n",
    "# G_hsr = nx.read_graphml(\"hsr_graph.graphml\")\n",
    "# ec_graph = from_networkx(G_ec)\n",
    "# hsr_graph = from_networkx(G_hsr)\n",
    "\n",
    "# # Shift node indices of ec Graph to ensure disjointness\n",
    "# offset = ec_graph.x.size(0)  # Number of nodes in ec graph \n",
    "# edge_index_ec = edge_index_ec + offset\n",
    "\n",
    "# # Combine edge indices and node features\n",
    "# combined_edge_index = torch.cat([hsr_graph.edge_index, edge_index_ec], dim=1)\n",
    "# combined_x = torch.cat([hsr_graph.x, ec_graph.x], dim=0)\n",
    "\n",
    "# # Create a new Data object for the combined graph\n",
    "# combined_graph = Data(edge_index=combined_edge_index, x=combined_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334ca0d2-5d7b-4376-ad2f-6c6a2fb966fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b14929-cddb-4497-b680-9092a736cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_hic = np.load('data/GBM39ec_5k_collapsed_matrix.npy')\n",
    "hsr_hic = np.load('data/GBM39HSR_5k_collapsed_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b90d67-4749-4060-a724-92d405a23a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_df = pd.read_csv('data/ec_cleaned.csv')\n",
    "hsr_df = pd.read_csv('data/hsr_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c30162-fe97-423e-9a39-ad1141bdc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsr_feats = torch.tensor(hsr_df[['read_count', 'total_genes']].to_numpy())\n",
    "hsr_labels = torch.zeros(hsr_feats.shape[0])\n",
    "\n",
    "ec_feats = torch.tensor(ec_df[['read_count', 'total_genes']].to_numpy())\n",
    "ec_labels = torch.ones(ec_feats.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b7f47d-1afc-49c3-bc03-5c63223cd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hic_to_sparse(hic_mat):\n",
    "    adj_mat = np.triu(hic_mat)\n",
    "    sparse_adj = coo_matrix(adj_mat)\n",
    "\n",
    "    return utils.from_scipy_sparse_matrix(sparse_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86d54f0-d048-48f9-857d-6fcc958d3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsr_edge_index, hsr_edge_attr = hic_to_sparse(hsr_hic)\n",
    "hsr_graph = torch_geometric.data.Data(edge_index = hsr_edge_index, edge_attr = hsr_edge_attr, x = hsr_feats, y = hsr_labels)\n",
    "\n",
    "ec_edge_index, ec_edge_attr = hic_to_sparse(ec_hic)\n",
    "ec_graph = torch_geometric.data.Data(edge_index = ec_edge_index, edge_attr = ec_edge_attr, x = ec_feats, y = ec_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d8567c-a615-4a6e-8786-15d1a6cb1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([ec_feats, hsr_feats], dim=0)\n",
    "hsr_edge_index = hsr_edge_index + ec_labels.shape[0]\n",
    "edge_index = torch.cat([ec_edge_index, hsr_edge_index], dim=1)\n",
    "edge_attr = torch.cat([ec_edge_attr, hsr_edge_attr], dim=0)\n",
    "labels = torch.cat([ec_labels, hsr_labels], dim=0)\n",
    "\n",
    "G = torch_geometric.data.Data(edge_index = edge_index, edge_attr = edge_attr, x = x, y = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f70a877-c4fb-4bc0-9287-9c0c9d0cdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_graph_conv_layers = num_graph_conv_layers\n",
    "        self.num_lin_layers = num_lin_layers\n",
    "        self.embeds = None\n",
    "\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = SAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = SAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "            self.conv3 = SAGEConv(graph_conv_layer_sizes[2], graph_conv_layer_sizes[3])\n",
    "        \n",
    "        if self.num_lin_layers == 1:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "        elif self.num_lin_layers == 2:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "        elif self.num_lin_layers == 3:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "            self.lin3 = nn.Linear(lin_hidden_sizes[2], lin_hidden_sizes[3])\n",
    "            \n",
    "        self.loss_calc = nn.CrossEntropyLoss()\n",
    "        self.torch_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        ### Graph convolution module\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv3(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            \n",
    "        #h = F.dropout(h, p = self.dropout_value)\n",
    "        scores = h\n",
    "        ### Linear module\n",
    "        if self.num_lin_layers == 0:\n",
    "            return scores\n",
    "        elif self.num_lin_layers == 1:\n",
    "            scores = self.lin1(scores)\n",
    "        elif self.num_lin_layers == 2:\n",
    "            scores = self.lin1(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "        elif self.num_lin_layers == 3:\n",
    "            scores = self.lin1(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin3(scores)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def loss(self, scores, labels):\n",
    "        xent_loss = self.loss_calc(scores, labels)\n",
    "        return xent_loss\n",
    "    \n",
    "    \n",
    "    def calc_softmax_pred(self, scores):\n",
    "        softmax = self.torch_softmax(scores)\n",
    "        predicted = torch.argmax(softmax, 1)\n",
    "        return softmax, predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9597e371-b0a0-46b2-a625-333e7bc340fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train/test sets\n",
    "def split_data(G, train_ratio=0.8, test_ratio=0.2, seed=42):\n",
    "    assert train_ratio + test_ratio == 1, \"Ratios must sum to 1.\"\n",
    "    \n",
    "    torch.manual_seed(seed)  # For reproducibility\n",
    "    \n",
    "    num_nodes = G.x.shape[0]  # Total number of nodes\n",
    "    indices = torch.randperm(num_nodes)  # Shuffle node indices randomly\n",
    "    \n",
    "    # Compute split size\n",
    "    train_size = int(train_ratio * num_nodes)\n",
    "    \n",
    "    # Assign indices to train and test sets\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "\n",
    "    # Create boolean masks\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    # Attach masks to the graph\n",
    "    G.train_mask = train_mask\n",
    "    G.test_mask = test_mask\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005193b9-b979-469d-a0ab-8f040943d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train function with train/test split, build test function - condensed\n",
    "def train(model, optimizer, graph, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    scores = model(graph)\n",
    "    loss = model.loss(scores[graph.train_mask], graph.y[graph.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), scores\n",
    "\n",
    "def test(model, graph, device):\n",
    "    model.eval()\n",
    "    scores = model(graph)\n",
    "    softmax, predicted = model.calc_softmax_pred(scores)\n",
    "    accs = []\n",
    "    for mask in [graph.train_mask, graph.test_mask]:\n",
    "        correct = (predicted[mask] == graph.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    \n",
    "    return tuple(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba3cfca1-f59e-4016-abef-d5c5e59510b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs + layer information\n",
    "num_features = G.num_node_features\n",
    "num_graph_sage_layers = 3\n",
    "num_classes = 2\n",
    "graph_sage_layer_sizes = [num_features,8,16,32]\n",
    "linear_layer_sizes = [32,16,8,num_classes]\n",
    "num_linear_layers = 3\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 1000\n",
    "#dropout_value = 0.5\n",
    "\n",
    "# Initialize the model, optimizer, and send to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#model.dropout_value = dropout_value\n",
    "\n",
    "# Assign train and test masks to graph\n",
    "G.x = G.x.float()\n",
    "G.y = G.y.long()\n",
    "G = split_data(G)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4008a1a9-967d-4c93-9db6-d799eeed5a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 2.8576, Train Acc: 0.6883, Test Acc: 0.6931\n",
      "Epoch 200, Loss: 2.0698, Train Acc: 0.7307, Test Acc: 0.7327\n",
      "Epoch 300, Loss: 0.0295, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 400, Loss: 0.0182, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 500, Loss: 0.0134, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 600, Loss: 0.0107, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 700, Loss: 0.0088, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 800, Loss: 0.0075, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 900, Loss: 0.0065, Train Acc: 1.0000, Test Acc: 0.9802\n",
      "Epoch 1000, Loss: 0.0058, Train Acc: 1.0000, Test Acc: 0.9802\n"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Training loop - Entire graph in one model\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss, _ = train(model, optimizer, G, device)\n",
    "    train_acc, test_acc = test(model, G, device)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == num_epochs:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73bde5-986a-4c24-97b3-04ef710e7bf3",
   "metadata": {},
   "source": [
    "With 2 hidden layers: sizes 32 and 64:\n",
    "- First ran with 200 epochs - loss bounced around a bit but didn't converge\n",
    "- Then ran with 2000 epochs - loss leveled out a bit around 1000, ended with about 88% train acc and 86% test acc. Similar to above with 5000 epochs but there is still obviously inconsistency in the training\n",
    "- 5000 epoch train - it eventually learns an even split and can achieve 100% accuracy. Going to try learning embeddings from separate models for each graph and classifying using lightgbm or xgboost\n",
    "\n",
    "With 3 hidden layers sizes 8, 16, and 32:\n",
    "- training is much more stable; loss decreases gradually\n",
    "- accuracy still quickly reaches 97+% and stays there. Maybe it found a set of parameters that very accurately differentiates ecDNA and HSR, or it is just figuring out which subgraph the test nodes come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8830e49-4aa4-4ac7-aea0-2fb5a9b74ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linear_layers = 0\n",
    "linear_layer_sizes = []\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 1000\n",
    "#dropout_value = 0.5\n",
    "\n",
    "# Initialize the model, optimizer, and send to device\n",
    "model_ec = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer_ec = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model_hsr = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer_hsr = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#model.dropout_value = dropout_value\n",
    "\n",
    "ec_graph.x = ec_graph.x.float()\n",
    "ec_labels = ec_graph.y\n",
    "ec_graph = ec_graph.to(device)\n",
    "\n",
    "hsr_graph.x = hsr_graph.x.float()\n",
    "hsr_labels = hsr_graph.y\n",
    "hsr_graph = hsr_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e499d55-59bb-48a5-b51c-2e21a4228fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def learn_embeddings(model, optimizer, graph, device):\n",
    "    scores = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss, scores = train(model, optimizer, G, device)\n",
    "        \n",
    "        if epoch % 100 == 0 or epoch == num_epochs:\n",
    "            print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86cde4c6-e44c-4820-8350-a95648c22614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 704.1915\n",
      "Epoch 200, Loss: 704.1915\n",
      "Epoch 300, Loss: 704.1915\n",
      "Epoch 400, Loss: 704.1915\n",
      "Epoch 500, Loss: 704.1915\n",
      "Epoch 600, Loss: 704.1915\n",
      "Epoch 700, Loss: 704.1915\n",
      "Epoch 800, Loss: 704.1915\n",
      "Epoch 900, Loss: 704.1915\n",
      "Epoch 1000, Loss: 704.1915\n",
      "Epoch 100, Loss: 1041.8651\n",
      "Epoch 200, Loss: 1041.8651\n",
      "Epoch 300, Loss: 1041.8651\n",
      "Epoch 400, Loss: 1041.8651\n",
      "Epoch 500, Loss: 1041.8651\n",
      "Epoch 600, Loss: 1041.8651\n",
      "Epoch 700, Loss: 1041.8651\n",
      "Epoch 800, Loss: 1041.8651\n",
      "Epoch 900, Loss: 1041.8651\n",
      "Epoch 1000, Loss: 1041.8651\n"
     ]
    }
   ],
   "source": [
    "ec_embeddings = learn_embeddings(model_ec, optimizer_ec, ec_graph, device)\n",
    "hsr_embeddings = learn_embeddings(model_hsr, optimizer_hsr, hsr_graph, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39b0f4-b08a-48e4-b59b-8064be1767a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not learning anything - confused on how to define loss so it can learn embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
