{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560b8c14-1449-42dd-9d62-72e21cd4c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdalquist/.local/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/mdalquist/.local/lib/python3.11/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/mdalquist/.local/lib/python3.11/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(\n",
      "/home/mdalquist/.local/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79264b7b-3d7e-49c1-8ebb-e42d101ecf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run - cell containing code for combining graph to two subparts with offset\n",
    "G_ec = nx.read_graphml(\"ec_graph.graphml\")\n",
    "G_hsr = nx.read_graphml(\"hsr_graph.graphml\")\n",
    "ec_graph = from_networkx(G_ec)\n",
    "hsr_graph = from_networkx(G_hsr)\n",
    "\n",
    "# Shift node indices of ec Graph to ensure disjointness\n",
    "offset = ec_graph.x.size(0)  # Number of nodes in ec graph \n",
    "edge_index_ec = edge_index_ec + offset\n",
    "\n",
    "# Combine edge indices and node features\n",
    "combined_edge_index = torch.cat([hsr_graph.edge_index, edge_index_ec], dim=1)\n",
    "combined_x = torch.cat([hsr_graph.x, ec_graph.x], dim=0)\n",
    "\n",
    "# Create a new Data object for the combined graph\n",
    "combined_graph = Data(edge_index=combined_edge_index, x=combined_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b14929-cddb-4497-b680-9092a736cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_hic = np.load('data/GBM39ec_5k_collapsed_matrix.npy')\n",
    "hsr_hic = np.load('data/GBM39HSR_5k_collapsed_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b90d67-4749-4060-a724-92d405a23a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_df = pd.read_csv('data/ec_cleaned.csv')\n",
    "hsr_df = pd.read_csv('data/hsr_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c30162-fe97-423e-9a39-ad1141bdc043",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsr_feats = torch.tensor(hsr_df[['read_count', 'total_genes']].to_numpy())\n",
    "hsr_labels = torch.zeros(hsr_feats.shape[0])\n",
    "\n",
    "ec_feats = torch.tensor(ec_df[['read_count', 'total_genes']].to_numpy())\n",
    "ec_labels = torch.ones(ec_feats.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b7f47d-1afc-49c3-bc03-5c63223cd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hic_to_sparse(hic_mat):\n",
    "    adj_mat = np.triu(hic_mat)\n",
    "    sparse_adj = coo_matrix(adj_mat)\n",
    "\n",
    "    return utils.from_scipy_sparse_matrix(sparse_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86d54f0-d048-48f9-857d-6fcc958d3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsr_edge_index, hsr_edge_attr = hic_to_sparse(hsr_hic)\n",
    "hsr_graph = torch_geometric.data.Data(edge_index = hsr_edge_index, edge_attr = hsr_edge_attr, x = hsr_feats, y = hsr_labels)\n",
    "\n",
    "ec_edge_index, ec_edge_attr = hic_to_sparse(ec_hic)\n",
    "ec_graph = torch_geometric.data.Data(edge_index = ec_edge_index, edge_attr = ec_edge_attr, x = ec_feats, y = ec_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d8567c-a615-4a6e-8786-15d1a6cb1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([ec_feats, hsr_feats], dim=0)\n",
    "hsr_edge_index = hsr_edge_index + ec_labels.shape[0]\n",
    "edge_index = torch.cat([ec_edge_index, hsr_edge_index], dim=1)\n",
    "edge_attr = torch.cat([ec_edge_attr, hsr_edge_attr], dim=0)\n",
    "labels = torch.cat([ec_labels, hsr_labels], dim=0)\n",
    "\n",
    "G = torch_geometric.data.Data(edge_index = edge_index, edge_attr = edge_attr, x = x, y = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f70a877-c4fb-4bc0-9287-9c0c9d0cdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_graph_conv_layers = num_graph_conv_layers\n",
    "        self.num_lin_layers = num_lin_layers\n",
    "\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = SAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            self.conv1 = SAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = SAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "            self.conv3 = SAGEConv(graph_conv_layer_sizes[2], graph_conv_layer_sizes[3])\n",
    "        \n",
    "        if self.num_lin_layers == 1:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "        elif self.num_lin_layers == 2:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "        elif self.num_lin_layers == 3:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "            self.lin3 = nn.Linear(lin_hidden_sizes[2], lin_hidden_sizes[3])\n",
    "            \n",
    "        self.loss_calc = nn.CrossEntropyLoss()\n",
    "        self.torch_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        ### Graph convolution module\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv3(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            \n",
    "        #h = F.dropout(h, p = self.dropout_value)\n",
    "        scores = h\n",
    "        ### Linear module\n",
    "        if self.num_lin_layers == 0:\n",
    "            return scores\n",
    "        elif self.num_lin_layers == 1:\n",
    "            scores = self.lin1(scores)\n",
    "        elif self.num_lin_layers == 2:\n",
    "            scores = self.lin1(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "        elif self.num_lin_layers == 3:\n",
    "            scores = self.lin1(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin3(scores)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def loss(self, scores, labels):\n",
    "        xent_loss = self.loss_calc(scores, labels)\n",
    "        return xent_loss\n",
    "    \n",
    "    \n",
    "    def calc_softmax_pred(self, scores):\n",
    "        softmax = self.torch_softmax(scores)\n",
    "        predicted = torch.argmax(softmax, 1)\n",
    "        return softmax, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f45ddec-70c4-4b5b-a971-aa762a80946d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a39b26-d5c3-44fe-a500-1eac88a64b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train function with train/test split, build test function - code from GC MERGE - do not run\n",
    "def to_cpu_npy(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "def train_model(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for classification task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch\n",
    "    train_AUROC_vec [array]: Training AUROC score for each epoch\n",
    "    valid_loss_vec [array]: Validation loss for each epoch\n",
    "    valid_AUROC_vec [array]: Validation AUROC score for each epoch\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "\n",
    "        train_softmax, _ = model.calc_softmax_pred(train_scores)\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_softmax, _ = model.calc_softmax_pred(valid_scores) \n",
    "\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_softmax = to_cpu_npy(train_softmax)\n",
    "        train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        valid_softmax = to_cpu_npy(valid_softmax)\n",
    "        valid_AUROC = roc_auc_score(valid_labels, valid_softmax[:,1], average=\"micro\")\n",
    "        \n",
    "        train_AUROC_vec[e] = train_AUROC\n",
    "        valid_AUROC_vec[e] = valid_AUROC\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec\n",
    "\n",
    "\n",
    "def eval_model(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained classification model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_AUROC [float]: Test set AUROC score;\n",
    "        analogous for train_AUROC (training set) and valid_AUPR (validation set)\n",
    "    test_AUPR [float]: Test set AUPR score\n",
    "        analogous for train_AUPR (training set) and valid_AUPR (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels;\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_softmax, test_pred = model.calc_softmax_pred(test_scores) \n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_pred = to_cpu_npy(test_pred)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_precision, test_recall, thresholds = precision_recall_curve(test_labels, test_softmax[:,1])\n",
    "    test_AUPR = auc(test_recall, test_precision)\n",
    "    # test_F1 = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    \n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_softmax, train_pred = model.calc_softmax_pred(train_scores) \n",
    "    train_pred = to_cpu_npy(train_pred)\n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_precision, train_recall, thresholds = precision_recall_curve(train_labels, train_softmax[:,1])\n",
    "    train_AUPR = auc(train_recall, train_precision)\n",
    "    # train_F1 = f1_score(train_labels, train_pred, average=\"micro\")\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_softmax, valid_pred = model.calc_softmax_pred(valid_scores) \n",
    "    valid_pred = to_cpu_npy(valid_pred)\n",
    "    valid_softmax = to_cpu_npy(valid_softmax)\n",
    "    valid_precision, valid_recall, thresholds = precision_recall_curve(valid_labels, valid_softmax[:,1])\n",
    "    valid_AUPR = auc(valid_recall, valid_precision)\n",
    "    # valid_F1 = f1_score(valid_labels, valid_pred, average=\"micro\")\n",
    "\n",
    "    return test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9597e371-b0a0-46b2-a625-333e7bc340fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train/test sets\n",
    "def split_data(G, train_ratio=0.8, test_ratio=0.2, seed=42):\n",
    "    assert train_ratio + test_ratio == 1, \"Ratios must sum to 1.\"\n",
    "    \n",
    "    torch.manual_seed(seed)  # For reproducibility\n",
    "    \n",
    "    num_nodes = G.x.shape[0]  # Total number of nodes\n",
    "    indices = torch.randperm(num_nodes)  # Shuffle node indices randomly\n",
    "    \n",
    "    # Compute split size\n",
    "    train_size = int(train_ratio * num_nodes)\n",
    "    \n",
    "    # Assign indices to train and test sets\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "\n",
    "    # Create boolean masks\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    # Attach masks to the graph\n",
    "    G.train_mask = train_mask\n",
    "    G.test_mask = test_mask\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "005193b9-b979-469d-a0ab-8f040943d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train function with train/test split, build test function - condensed\n",
    "def train(model, optimizer, graph, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    scores = model(graph)\n",
    "    loss = model.loss(scores[graph.train_mask], graph.y[graph.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), scores\n",
    "\n",
    "def test(model, graph, device):\n",
    "    model.eval()\n",
    "    scores = model(graph)\n",
    "    softmax, predicted = model.calc_softmax_pred(scores)\n",
    "    accs = []\n",
    "    for mask in [graph.train_mask, graph.test_mask]:\n",
    "        correct = (predicted[mask] == graph.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba3cfca1-f59e-4016-abef-d5c5e59510b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs + layer information\n",
    "num_features = G.num_node_features\n",
    "num_graph_sage_layers = 3\n",
    "num_classes = 2\n",
    "graph_sage_layer_sizes = [num_features,8,16,32]\n",
    "linear_layer_sizes = [32,16,8,num_classes]\n",
    "num_linear_layers = 3\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 1000\n",
    "#dropout_value = 0.5\n",
    "\n",
    "# Initialize the model, optimizer, and send to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#model.dropout_value = dropout_value\n",
    "\n",
    "# Assign train and test masks to graph\n",
    "G.x = G.x.float()\n",
    "G.y = G.y.long()\n",
    "G = split_data(G)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4008a1a9-967d-4c93-9db6-d799eeed5a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 4.1010, Train Acc: 0.6908, Test Acc: 0.6436\n",
      "Epoch 200, Loss: 2.8036, Train Acc: 0.6958, Test Acc: 0.6832\n",
      "Epoch 300, Loss: 1.7045, Train Acc: 0.7057, Test Acc: 0.6931\n",
      "Epoch 400, Loss: 0.2854, Train Acc: 0.9451, Test Acc: 0.9703\n",
      "Epoch 500, Loss: 0.0655, Train Acc: 0.9925, Test Acc: 0.9703\n",
      "Epoch 600, Loss: 0.0376, Train Acc: 0.9925, Test Acc: 0.9703\n",
      "Epoch 700, Loss: 0.0353, Train Acc: 0.9925, Test Acc: 0.9703\n",
      "Epoch 800, Loss: 0.0338, Train Acc: 0.9925, Test Acc: 0.9703\n",
      "Epoch 900, Loss: 0.0327, Train Acc: 0.9925, Test Acc: 0.9703\n",
      "Epoch 1000, Loss: 0.0320, Train Acc: 0.9925, Test Acc: 0.9703\n"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Training loop - Entire graph in one model\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(model, optimizer, G, device)\n",
    "    train_acc, test_acc = test(model, G, device)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch == num_epochs:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a73bde5-986a-4c24-97b3-04ef710e7bf3",
   "metadata": {},
   "source": [
    "With 2 hidden layers: sizes 32 and 64:\n",
    "- First ran with 200 epochs - loss bounced around a bit but didn't converge\n",
    "- Then ran with 2000 epochs - loss leveled out a bit around 1000, ended with about 88% train acc and 86% test acc. Similar to above with 5000 epochs but there is still obviously inconsistency in the training\n",
    "- 5000 epoch train - it eventually learns an even split and can achieve 100% accuracy. Going to try learning embeddings from separate models for each graph and classifying using lightgbm or xgboost\n",
    "\n",
    "With 3 hidden layers sizes 8, 16, and 32:\n",
    "- training is much more stable; loss decreases gradually\n",
    "- accuracy still quickly reaches 97+% and stays there. Maybe it found a set of parameters that very accurately differentiates ecDNA and HSR, or it is just figuring out which subgraph the test nodes come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8830e49-4aa4-4ac7-aea0-2fb5a9b74ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linear_layers = 0\n",
    "linear_layer_sizes = []\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 1000\n",
    "#dropout_value = 0.5\n",
    "\n",
    "# Initialize the model, optimizer, and send to device\n",
    "model_ec = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer_ec = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model_hsr = GraphSAGE(num_features, num_graph_sage_layers, graph_sage_layer_sizes, linear_layer_sizes, num_linear_layers, num_classes).to(device)\n",
    "optimizer_hsr = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#model.dropout_value = dropout_value\n",
    "\n",
    "ec_graph.x = ec_graph.x.float()\n",
    "ec_labels = ec_graph.y\n",
    "ec_graph = ec_graph.to(device)\n",
    "\n",
    "hsr_graph.x = hsr_graph.x.float()\n",
    "hsr_labels = hsr_graph.y\n",
    "hsr_graph = hsr_graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e499d55-59bb-48a5-b51c-2e21a4228fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def learn_embeddings(model, optimizer, graph, device):\n",
    "    scores = 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss, scores = train(model, optimizer, G, device)\n",
    "        \n",
    "        if epoch % 100 == 0 or epoch == num_epochs:\n",
    "            print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86cde4c6-e44c-4820-8350-a95648c22614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 704.1915\n",
      "Epoch 200, Loss: 704.1915\n",
      "Epoch 300, Loss: 704.1915\n",
      "Epoch 400, Loss: 704.1915\n",
      "Epoch 500, Loss: 704.1915\n",
      "Epoch 600, Loss: 704.1915\n",
      "Epoch 700, Loss: 704.1915\n",
      "Epoch 800, Loss: 704.1915\n",
      "Epoch 900, Loss: 704.1915\n",
      "Epoch 1000, Loss: 704.1915\n",
      "Epoch 100, Loss: 1041.8651\n",
      "Epoch 200, Loss: 1041.8651\n",
      "Epoch 300, Loss: 1041.8651\n",
      "Epoch 400, Loss: 1041.8651\n",
      "Epoch 500, Loss: 1041.8651\n",
      "Epoch 600, Loss: 1041.8651\n",
      "Epoch 700, Loss: 1041.8651\n",
      "Epoch 800, Loss: 1041.8651\n",
      "Epoch 900, Loss: 1041.8651\n",
      "Epoch 1000, Loss: 1041.8651\n"
     ]
    }
   ],
   "source": [
    "ec_embeddings = learn_embeddings(model_ec, optimizer_ec, ec_graph, device)\n",
    "hsr_embeddings = learn_embeddings(model_hsr, optimizer_hsr, hsr_graph, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39b0f4-b08a-48e4-b59b-8064be1767a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not learning anything - confused on how to define loss so it can learn embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
